[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Programa",
    "section": "",
    "text": "Prof. Juan Carlos Castillo\n   325 Sociología FACSO, Universidad de Chile\n   juancastillov@uchile.cl\n   Agendar reunión\n\n\n\n\n\n   Lunes y Martes\n   7 Agosto al 28 de Noviembre, 2023\n   8:30-10:00 (Lunes) y 10:15-11:45 (Martes)\n   Lunes - Aulario A Salas 7 y 8 – Martes - Online o Laboratorio de Computación 2\n   Slack"
  },
  {
    "objectID": "syllabus.html#sobre-el-sentido-general-del-curso",
    "href": "syllabus.html#sobre-el-sentido-general-del-curso",
    "title": "Programa",
    "section": "Sobre el sentido general del curso",
    "text": "Sobre el sentido general del curso\nEn este curso vamos a aprender tres cosas principales:\n\ninferencia: los resultados que encontramos en nuestra muestra, ¿se encuentran también en la población de la cual proviene la muestra?\nmedidas de asociación entre variables: tamaño y significación estadística\nreporte y reproducibilidad de los análisis estadísticos: nuestros análisis se reflejan en productos como tablas y gráficos. No basta con entenderlos e interpretarlos, sino también es fundamental una buena comunicación."
  },
  {
    "objectID": "syllabus.html#propósito-general-del-curso",
    "href": "syllabus.html#propósito-general-del-curso",
    "title": "Programa",
    "section": "Propósito general del curso",
    "text": "Propósito general del curso\nAl finalizar el curso los estudiantes conocerán los fundamentos del análisis estadístico inferencial. Se espera que los estudiantes sean capaces de:\n\nelaborar de manera pertinente hipótesis estadísticas\naplicar estadísticos de asociación y correlación, a partir de los cuáles puedan desarrollar análisis de problemas sociales\ncorroborar el cumplimiento de las condiciones de aplicación de cada estadístico\nutilizar diferentes softwares de análisis estadístico\ncontrastar hipótesis de investigación\nelaborar conclusiones integrando fundamentos teóricos con herramientas de análisis estadístico de resultados.\n\nComplementariamente se espera que los estudiantes adquieran herramientas que les permitan comunicar resultados de investigación en contextos sociales, profesionales y académicos."
  },
  {
    "objectID": "syllabus.html#competencias",
    "href": "syllabus.html#competencias",
    "title": "Programa",
    "section": "Competencias",
    "text": "Competencias\n1a. Delimitar, conceptualizar y analizar diversos objetos de investigación social, con especial énfasis en aquellos relacionados con los procesos de transformación del país y Latinoamérica\n1b. Manejar diversas estrategias metodológicas de las ciencias sociales\n1c. Manejar un conjunto de herramientas para el procesamiento y análisis de información\n1d. Transmitir los conocimientos derivados de la práctica investigativa, así como aquellos adquiridos durante el proceso formativo."
  },
  {
    "objectID": "syllabus.html#subcompetencias",
    "href": "syllabus.html#subcompetencias",
    "title": "Programa",
    "section": "Subcompetencias",
    "text": "Subcompetencias\n\n1.4 Contribuir a generar conocimiento sociológico en el marco de estudios y/o procesos de investigación donde se articulen creativamente las dimensiones teórica, metodológica y práctica.\n1.5 Comunicar los saberes disciplinares de manera pertinente a las características de distintos contextos y audiencias, utilizando diversas estrategias y formatos."
  },
  {
    "objectID": "syllabus.html#resultados-del-aprendizaje",
    "href": "syllabus.html#resultados-del-aprendizaje",
    "title": "Programa",
    "section": "Resultados del aprendizaje",
    "text": "Resultados del aprendizaje\n\nComprende, domina y es capaz de explicar los elementos conceptuales subyacentes a la determinación de la asociación poblacional entre dos variables a partir del análisis de una muestra, y es capaz de traducir hipótesis derivadas de la teoría sociológica en hipótesis estadísticas posibles de contrastar empíricamente con los datos.\nEs capaz de seleccionar y usar herramientas estadísticas adecuadas para evaluar la asociación entre dos variables considerando las características de los datos y las condiciones de aplicación de cada técnica.\nLogra interpretar desde un punto de vista estadístico y sociológico los resultados derivados de pruebas estadísticas para analizar la relación entre dos variables.\nEs capaz de reportar y comunicar adecuada y eficientemente los resultados de los análisis estadísticos"
  },
  {
    "objectID": "syllabus.html#saberes-contenidos",
    "href": "syllabus.html#saberes-contenidos",
    "title": "Programa",
    "section": "Saberes / Contenidos",
    "text": "Saberes / Contenidos\n\nUnidad I: Asociación entre dos variables cuantitativas.\n\nDescripción, asociación, y explicación en estadística. La importancia del análisis de covariación entre variables. La inferencia estadística: tipos y rol en el análisis descriptivo, asociativo y explicativo.\nConcepto de covarianza y relación/correlación lineal versus relación no-lineal entre variables.\nCorrelación de Pearson: supuestos y condiciones de aplicación, interpretación de resultados como tamaño efecto según criterios de Cohen. El coeficiente de determinación, utilidad e interpretación.\n\n\n\nUnidad II: Inferencia y asociación\n\nAsociación entre una variable cuantitativa y una dicotómica como intervalos de confianza de medias y proporciones usando distribución Z y revisión de conceptos fundamentales de inferencia: área de una distribución, probabilidades en la curva normal, error Tipo I, y error tipo II.\nAsociación entre una variable cuantitativa y una dicotómica como diferencia de medias y proporciones con prueba Z; revisión y análisis de valor de Z observado, el concepto de valor-p y determinación del tamaño del efecto con d de Cohen.\nGeneralización de intervalos de confianza y pruebas de diferencia de medias y proporciones en dos grupos en muestras pequeñas usando distribución t de Student; revisión de concepto de grados de libertad.\nGeneralizaciones de esta lógica: Prueba t de Student e intervalos de confianza en correlaciones de Pearson; Comparación de medias de tres o más grupos con ANOVA e intervalos de confianza corregidos por número de comparaciones.\n\n\n\nUnidad III: Asociación con variables categóricas\n\nEl uso de tablas de contingencia y la determinación de la asociación: uso de frecuencias observadas, porcentajes fila, columna y totales.\nEvaluación de la asociación poblacional mediante Chi Cuadrado: el concepto de frecuencias esperadas y observadas. Estimación e interpretación de resultados y medición de tamaño efecto.\nBreve mirada a estadísticos de correlación para variables categóricas: la correlación tetracórica y policórica."
  },
  {
    "objectID": "syllabus.html#metodología",
    "href": "syllabus.html#metodología",
    "title": "Programa",
    "section": "Metodología",
    "text": "Metodología\n\nSesiones de clases lectivas presenciales semanales, donde se presentarán los aspectos centrales de los contenidos correspondientes a la semana.\nPrácticos online semanales: cada tema de las sesiones se acompaña de una guía práctica de aplicación de contenidos. Estas guías están diseñadas para ser desarrolladas de manera autónoma por cada estudiante semana a semana. También serán desarrolladas y revisadas cada semana en grupos pequeños con supervisión de ayudantes para dar mayor oportunidad de participación y resolver las dudas respectivas. Existe un reporte de progreso asociado a estas guías que deberá ser completado semanalmente con fines de monitoreo y retroalimentación.\nTrabajos: se desarrollarán trabajos de investigación que permitirán a l_s participantes aplicar contenidos y recibir retroalimentación de su desempeño. Los trabajos serán asesorados por ayudantes que se asignarán a cada grupo."
  },
  {
    "objectID": "syllabus.html#evaluación",
    "href": "syllabus.html#evaluación",
    "title": "Programa",
    "section": "Evaluación",
    "text": "Evaluación\nEl curso tendrá tres instancias de evaluación:\n\nEvaluación 1: prueba correlación lineal (30%).\nEvaluación 2: prueba inferencia estadística (30%)\nEvaluación 3: trabajo grupal (40%)\n\nLa nota ponderada de las evaluaciones equivaldrá al 60% de la nota del curso y el examen final al 40% restante."
  },
  {
    "objectID": "syllabus.html#inasistencias-y-atraso-en-entregas",
    "href": "syllabus.html#inasistencias-y-atraso-en-entregas",
    "title": "Programa",
    "section": "Inasistencias y atraso en entregas",
    "text": "Inasistencias y atraso en entregas\nLos justificativos por ausencia o atraso se realizan en la secretaría de carrera. Lo que la carrera informe como justificado, es lo que se va a considerar en el curso. No enviar justificativos a equipo docente y a ayudantes directamente, no es necesario ni apropiado para l_s estudiantes tener que exponer situaciones personales.\nEn caso de faltar a alguna de las evaluaciones existirá una única fecha para evaluaciones recuperativas. Si en esa fecha no es posible asistir por motivos justificados, entonces pasará directo a examen.\nEn el caso de los trabajos, en caso de atraso se descontará 0.5 por día adicional. Si el trabajo no se entrega luego del tercer día de atraso será calificado con nota 1.0"
  },
  {
    "objectID": "syllabus.html#requisitos-de-aprobación",
    "href": "syllabus.html#requisitos-de-aprobación",
    "title": "Programa",
    "section": "Requisitos de aprobación",
    "text": "Requisitos de aprobación\nRequisitos de eximición a examen:\n\ncontar con un promedio ponderado igual o superior a 5.5\nno tener nota bajo 4.0 en ninguna de las evaluaciones\n\nRequisitos para presentación a examen:\n\nPodrán presentarse al examen de primera oportunidad los estudiantes que hayan obtenido una calificación final igual o superior a 3.5.\nEl examen de segunda oportunidad será para aquellos estudiantes que presenten una nota igual o inferior a 3.5 o aquellos que en el examen de primera oportunidad no hubiesen logrado una nota igual o superior a 4.0."
  },
  {
    "objectID": "syllabus.html#bibliografía-obligatoria",
    "href": "syllabus.html#bibliografía-obligatoria",
    "title": "Programa",
    "section": "Bibliografía Obligatoria",
    "text": "Bibliografía Obligatoria\nCapítulos correspondientes a cada sesión de los siguientes textos principales:\n\nRitchey, F. (2008) Estadística para las ciencias sociales. McGraw-Hill: México.\nMoore (2010) Estadística aplicada básica. Barcelona: Antoni Bosch.\nPardo, Ruiz y San Martín (2015). Análisis de Datos en Ciencias Sociales y de la Salud I. Editorial Síntesis: Madrid."
  },
  {
    "objectID": "syllabus.html#bibliografía-complementaria",
    "href": "syllabus.html#bibliografía-complementaria",
    "title": "Programa",
    "section": "Bibliografía Complementaria",
    "text": "Bibliografía Complementaria\nWickham, H., & Grolemund, G. (2016). R for data science: import, tidy, transform, visualize, and model data (First edition). Sebastopol: O’Reilly.\nField, A., Milles, J., & Field, Z. (2012). Discovering statistics using R. London: Sage.\nSalkind, N. J. (Ed.). (2010). Encyclopedia of research design (Vol. 1). Sage.\nLevin, J. & Levin, W. (1997). Fundamentos de Estadística en la Investigación Social (Vol.2). Oxford University Press."
  },
  {
    "objectID": "syllabus.html#varios",
    "href": "syllabus.html#varios",
    "title": "Programa",
    "section": "VARIOS",
    "text": "VARIOS\n\nSe espera asistencia y participación activa, tanto a las sesiones lectivas como a las prácticas. Se pasará lista en todas las sesiones. No habrá penalización por inasistencia, pero si llevaremos registro principalmente con objetivos de monitoreo y retroalimentación del curso\nLas clases en general se acompañan de documentos de presentación, que estarán disponibles antes de la sesión en la página de Contenidos, y están desarrollados con base en Rmarkdown/XaringanRmarkdown/ Xaringan. Estos documentos no son:\n\n“la clase”\nautoexplicativos (ni aspiran a serlo)\n“el ppt” (ni “la ppt”)\n\nPolíticas de participación y trato: se espera y enfatiza la participación por distintos canales disponibles. También se enfatiza un trato respetuoso y horizontal. Quienes están tomando este curso serán referidos como participantes y/o estudiantes, evitar el uso de “l_s cabr_s” o “l_s chiquill_s”, que si bien puede intentar transmitir cercanía finalmente expresan minimización de la contraparte. Quien no se sienta tratad_ apropiadamente o vea que otr_s no lo están siendo, se solicita reportar para solucionar la situación."
  },
  {
    "objectID": "syllabus.html#programación-de-sesiones",
    "href": "syllabus.html#programación-de-sesiones",
    "title": "Programa",
    "section": "Programación de sesiones",
    "text": "Programación de sesiones\nVisitar la página de Planificación."
  },
  {
    "objectID": "news/index.html",
    "href": "news/index.html",
    "title": "Noticias",
    "section": "",
    "text": "Ordenar por\n       Por defecto\n         \n          Fecha - Menos reciente\n        \n         \n          Fecha - Más reciente\n        \n         \n          Título\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nFecha\n\n\nTítulo\n\n\nCategorías\n\n\n\n\n\n\nlunes agosto 7, 2023 at 12:00 AM\n\n\nBienvenid_s a clases!\n\n\ncomenzando\n\n\n\n\nlunes agosto 7, 2023 at 12:00 AM\n\n\nInformaciones de la semana\n\n\ninfo\n\n\n\n\n\n\nNo hay resultados\n\n\n\n\n\n\n\n\nSuscribirse!\n\n\n\nPuedes usar un lector de feeds como Feedly o un servicio RSS-to-email como Blogtrottr para suscribirte a cualquiera de estos mensajes. ::: {.grid}\n\n\n RSS\n\n\n\n\n\n\n\n\n\n:::"
  },
  {
    "objectID": "news/2023-01-09_welcome.html",
    "href": "news/2023-01-09_welcome.html",
    "title": "Bienvenid_s a clases!",
    "section": "",
    "text": "← News\n\n\n\nHola a todos!"
  },
  {
    "objectID": "example/index.html",
    "href": "example/index.html",
    "title": "Code examples",
    "section": "",
    "text": "Visit this section after you have finished the readings and lecture videos. It contains fully annotated R code and other supplementary information and it will be indispensable as you work on your problem sets and project.\nMany sections also contain videos of me live coding the examples so you can see what it looks like to work with R in real time. You’ll notice me make all sorts of little errors, which is totally normal—everyone does!"
  },
  {
    "objectID": "content/index.html",
    "href": "content/index.html",
    "title": "Sobre clases y lecturas",
    "section": "",
    "text": "Cada clase tiene como referencia lecturas que deben completarse antes de la sesión correspondiente.\nEn esta sección se encuentran disponibles los documentos de presentación que sirven de base a cada clase. Para verlos en pantalla completa presionar F sobre el documento."
  },
  {
    "objectID": "content/05-content.html",
    "href": "content/05-content.html",
    "title": "Inferencia 1 - Introducción: Probabilidades y curva normal",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "content/03-content.html",
    "href": "content/03-content.html",
    "title": "Asociación 2: Correlación de Pearson",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "content/01-content.html",
    "href": "content/01-content.html",
    "title": "Presentación",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "assignment/05-practico.html",
    "href": "assignment/05-practico.html",
    "title": "Reportes dinámicos 2",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender cómo crear y mostrar tablas y gráficos en documentos dinámicos mediante R Markdown. Además, aprenderemos cómo autoreferenciar elementos dentro de nuestro documento Rmd.\nEn detalle, aprenderemos:\n\nGeneración y presentación de tablas en R Markdown.\nGeneración y presentación de gráficos en R Markdown.\nCómo autoreferenciar elementos dentro un documento R Markdown.\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados derivados de las encuestas realizadas en diferentes países por el Latin American Public Opinion Proyect (LAPOP) en su ola del 2018. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  LAPOP 2018. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos LAPOP 2018."
  },
  {
    "objectID": "assignment/05-practico.html#recursos-de-la-práctica",
    "href": "assignment/05-practico.html#recursos-de-la-práctica",
    "title": "Reportes dinámicos 2",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados derivados de las encuestas realizadas en diferentes países por el Latin American Public Opinion Proyect (LAPOP) en su ola del 2018. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  LAPOP 2018. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos LAPOP 2018."
  },
  {
    "objectID": "assignment/05-practico.html#chunks",
    "href": "assignment/05-practico.html#chunks",
    "title": "Reportes dinámicos 2",
    "section": "Chunks",
    "text": "Chunks\nPara integrar código de R en un archivo RMarkdown usamos los chunks, que son trozos de código dentro de nuestra hoja. Estos permiten hacer análisis dentro del documento visualizando los resultados en el documento final.\nLos chunks se ven así dentro del .Rmd:\n```{r}\n# El codigo va aquí\n\n```\n\nInsertar chunks\nHay tres formas de insertar chunks:\n\nPulsar ⌘⌥I en macOS o Control + Alt + I en Windows\nPulsa el botón “Insert” en la parte superior de la ventana del editor\n\n\n\n\n\n\n\nEscribirlo manualmente (no recomendado)\n\n\n\nNombre de chunk\nPara añadir un nombre, inclúyelo inmediatamente después de la {r en la primera línea del chunk. Los nombres no pueden contener espacios, pero sí guiones bajos y guiones.\nImportante: Todos los nombres de chunk de tu documento deben ser únicos.\n```{r nombre-chunk}\n# El codigo va aquí\n\n```\n\n\nOpciones de chunk\nHay distintas opciones diferentes que puedes establecer para cada chunk. Puedes ver una lista completa en la Guía de referencia de RMarkdown o en el sitio web de knitr.\nLas opciones van dentro de la sección {r} del chunk:\n```{r nombre-chunk, message = FALSE, echo = TRUE}\n# El codigo va aquí\n\n```\nOtra forma de hacerlo es configurar las opciones generales de todos los chunks que hagamos al inicio del documento:\n```{r setup, include = FALSE}\nknitr::opts_chunk$set(echo = TRUE, \n                      message = FALSE, \n                      warning = FALSE)\n\n```\nDe esta manera ya no es necesario indicar en cada chunk las opciones, y se aplicaran las configuraciones generales que indicamos al comienzo."
  },
  {
    "objectID": "assignment/03-practico.html",
    "href": "assignment/03-practico.html",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "El objetivo de esta guía práctica es conocer maneras de reportar coeficientes de correlación y cómo interpretar sus tamaños de efecto en ciencias sociales. Además, nos introduciremos en el tratamiento de valores perdidos y otras medidas de correlación entre variables.\nEn detalle, aprenderemos:\n\nCómo reportar y presentar matrices de correlación.\nInterpretar el tamaño de efecto de una correlación.\nTratamiento de casos perdidos.\nQué es y cómo calcular la correlación de Spearman.\nQué es el coeficiente de determinación \\(R^2\\).\n\n\n\n\n\n\n\nNota\n\n\n\n¿Qué era la correlación?\nLa correlación es una medida de asociación entre variables, que describe el sentido (dirección) y fuerza de la asociación.\nEn otras palabras, nos permite conocer cómo y cuánto se relaciona la variación de una variable, con la variación de otra variable.\n\n\n\n\nEn esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "assignment/03-practico.html#recursos-de-la-práctica",
    "href": "assignment/03-practico.html#recursos-de-la-práctica",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "",
    "text": "En esta práctica trabajaremos con un subconjunto de datos previamente procesados del Estudio Longitudinal Social de Chile (ELSOC) del año 2016, elaborado por COES. Para este ejercicio, obtendremos directamente esta base desde internet. No obstante, también tienes la opción de acceder a la misma información a través del siguiente enlace:  ELSOC 2016. Desde allí, podrás descargar el archivo que contiene el subconjunto procesado de la base de datos ELSOC 2016."
  },
  {
    "objectID": "assignment/03-practico.html#tratamiento-de-casos-perdidos",
    "href": "assignment/03-practico.html#tratamiento-de-casos-perdidos",
    "title": "Matrices de correlación y tamaños de efecto",
    "section": "Tratamiento de casos perdidos",
    "text": "Tratamiento de casos perdidos\nTrabajar con datos a menudo implica enfrentar valores perdidos (NA), lo que puede ser un gran desafío. Estos valores indican la ausencia de un valor en una base de datos. Los valores perdidos pueden originarse por diversas razones, como el sesgo de no respuesta en encuestas, errores en la entrada de datos o simplemente la falta de información para ciertas variables.\n\n\n\n\n\n\nX1\nX2\nX3\nX4\n\n\n\n\nNA\n4\n1\nHola\n\n\n7\n1\n4\nNo soy un NA\n\n\n8\nNA\n2\nNA\n\n\n9\nNA\n9\nAmo R\n\n\n3\n3\n6\nNA\n\n\n\n\n\n\n\n\nLa presencia de valores perdidos puede tener un impacto considerable en la precisión y confiabilidad de los análisis estadísticos, lo que a su vez puede conducir a resultados sesgados y conclusiones incorrectas.\nExisten varias formas de tratar valores perdidos, que van desde enfoques simples hasta métodos más complejos, como la imputación. En esta ocasión, nos centraremos en las dos estrategias más comunes:\n\ntrabajar exclusivamente con casos completos (listwise) o\nretener los casos con valores perdidos, pero excluyéndolos al calcular estadísticas (pairwise).\n\n\na) Analísis con casos completos: listwise deletion\nEste enfoque es uno de los más conocidos: implica remover completamente las observaciones que tienen valores perdidos en cualquier variable de interés. En otras palabras, si una fila/caso en un conjunto de datos tiene al menos un valor faltante en alguna de las variables que estás considerando, se eliminará por completo.\nEn R, esto podemos hacerlo con la función na.omit. Para hacer esto, sigamos estos pasos:\n\nrespaldar la base de datos original en el espacio de trabajo (por si queremos en adelante realizar algún análisis referido a casos perdidos)-\ncontamos el número de casos con el comando dim.\ncontamos cuántos y en dónde tenemos casos perdidos.\nborramos los casos perdidos con na.omit.\ncontamos nuevamente con dim para asegurarnos que se borraron.\n\n\nproc_elsoc_original &lt;- proc_elsoc\ndim(proc_elsoc)\n\n[1] 2927    7\n\n\n\nsum(is.na(proc_elsoc))\n\n[1] 81\n\n\n\ncolSums(is.na(proc_elsoc))\n\nmesfuerzo  mtalento       ess    edcine      sexo      edad    pmerit \n       18        20        12         2         0         0        29 \n\n\n\nproc_elsoc &lt;- na.omit(proc_elsoc)\ndim(proc_elsoc)\n\n[1] 2887    7\n\n\nAhora nos quedamos con 2887 observaciones sin casos perdidos.\nAunque simple de implementar, con este enfoque podemos perder información importante, especialmente si los valores perdidos no se distribuyen aleatoriamente.\n\nSiempre hay que intentar rescatar la mayor cantidad de casos posibles. Por lo tanto, si un listwise genera más de un 10% de casos perdidos se debe detectar qué variables esta produciendo esta pérdida e intentar recuperar datos. Puedes revisar un ejemplo aquí.\n\n\n\nb) Retener pero excluir: pairwise deletion\nA diferencia del anterior, este es un enfoque en el que las observaciones se utilizan para el análisis siempre que tengan datos disponibles para las variables específicas que se están analizando. En lugar de eliminar toda una fila si falta un valor, se eliminan solo los valores faltantes en las variables que se están analizando en ese momento.\nPara hacer esto en R debemos siempre verificar e indicar en nuestro código si queremos (o no) remover los NA para realizar los análisis.\n\nmean(proc_elsoc_original$pmerit); mean(proc_elsoc$edad); mean(proc_elsoc$ess)\n\n[1] NA\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\nmean(proc_elsoc_original$pmerit, na.rm = TRUE); mean(proc_elsoc$edad, na.rm = TRUE); mean(proc_elsoc$ess, na.rm = TRUE)\n\n[1] 2.653899\n\n\n[1] 45.98337\n\n\n[1] 4.333564\n\n\nCon el primer código no obtuvimos información sustantiva en ciertas variables, pero con el segundo sí al remover los NA solo de dicha variable para un cálculo determinado."
  },
  {
    "objectID": "assignment/01-practico.html",
    "href": "assignment/01-practico.html",
    "title": "Reportes dinámicos con RMarkdown",
    "section": "",
    "text": "El objetivo de esta guía práctica es familiarizarnos con las herramientas fundamentales para crear documentos dinámicos que nos permitan combinar texto y análisis (código) en un único reporte utilizando RMarkdown.\nEn detalle, aprenderemos:\n\nQué es la reproducibilidad y un flujo de trabajo reproducible\nQué es escritura abierta con Markdown\nQué es RMarkdown y cómo generar reportes dinámicos"
  },
  {
    "objectID": "assignment/01-practico.html#formatos-básicos-de-markdown",
    "href": "assignment/01-practico.html#formatos-básicos-de-markdown",
    "title": "Reportes dinámicos con RMarkdown",
    "section": "Formatos básicos de Markdown",
    "text": "Formatos básicos de Markdown\n\n\n\nEscribe…\n…o…\n…para obtener\n\n\n\n\nAlgo de texto en el párrafo.\n\nMás texto\nespacio entre lineas.\n\nAlgo de texto.\nAlgo de texto en el párrafo. Siempre utilizando espacios para dividir párrafos\n\n\n`*Cursivas*`\n`_Cursivas_`\nCursivas\n\n\n`**Negrita**`\n`__Negrita__`\nNegrita\n\n\n# Título 1\n\nTítulo 1\n\n\n## Título 2\n\nTítulo 2\n\n\n### Título 3\n\nTítulo 3\n\n\n(puedes llegar hasta un título N° 6 con ######)\n\n\n\n\n`[Link text](http://www.example.com)`\n\nLink text\n\n\n`![Image caption](/path/to/image.png)`\n\n\n\n\n` ```Inline code``` ` with backticks\n\nInline code with backticks\n\n\n&gt; Citas\n\n\nCitas\n\n\n\n- Cosas en\n- listas\n- desordenadas\n* Cosas en\n* listas\n* desordenadas\n\nCosas en\nlistas\ndesordenadas\n\n\n\n1. Cosas en\n2. listas\n3. ordenadas\n1) Cosas en\n2) listas\n3) ordenadas\n\nCosas en\nlistas\nordenadas\n\n\n\nLínea horizontal\n\n---\nLínea horizontal\n\n***\nLínea horizontal"
  },
  {
    "objectID": "assignment/02-practico.html",
    "href": "assignment/02-practico.html",
    "title": "Cálculo y reporte de correlación",
    "section": "",
    "text": "El objetivo de esta guía práctica es aprender a calcular y graficar la correlación entre dos variables utilizando R.\nEn detalle, aprenderemos:\n\nQué es una correlación\nCuál es la correlación de Pearson\nCómo calcular una correlación de Pearson y graficarla"
  },
  {
    "objectID": "assignment/02-practico.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "href": "assignment/02-practico.html#diagrama-de-dispersión-nube-de-puntos-o-scatterplot",
    "title": "Cálculo y reporte de correlación",
    "section": "Diagrama de dispersión (nube de puntos o scatterplot)",
    "text": "Diagrama de dispersión (nube de puntos o scatterplot)\nSiempre es recomendable acompañar el valor de la correlación con una exploración gráfica de la distribución bivariada de los datos. El gráfico o diagrama de dispersión es una buena herramienta, ya que muestra la forma, la dirección y la fuerza de la relación entre dos variables cuantitativas.\nEste tipo de gráfico lo podemos realizar usando la librería ggplot2.\n\npacman::p_load(ggplot2)\nplot1 &lt;- ggplot(data, \n                aes(x=educ, y=ing)) + \n                geom_point(colour = \"red\", \n                size = 5)\nplot1\n\n\n\n\nEn el gráfico podemos ver como se crea una nube de puntos en las intersecciones de los valores para ambas variables de cada caso."
  },
  {
    "objectID": "assignment/02-practico.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "href": "assignment/02-practico.html#el-cuarteto-de-anscombe-y-las-limitaciones-de-la-correlación-lineal",
    "title": "Cálculo y reporte de correlación",
    "section": "El cuarteto de Anscombe y las limitaciones de la correlación lineal",
    "text": "El cuarteto de Anscombe y las limitaciones de la correlación lineal\nAhora, revisaremos un muy buen ejemplo de la importancia de la exploración gráfica de los datos mediante un ejemplo de Anscombe (1973), que permite visualizar las limitaciones del coeficiente de correlación.\nPrimero, crearemos la base de datos:\n\nanscombe &lt;- data.frame(\n  x1 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y1 = c(8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68),\n  x2 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y2 = c(9.14, 8.14, 8.74, 8.77, 9.26, 8.10, 6.13, 3.10, 9.13, 7.26, 4.74),\n  x3 = c(10, 8, 13, 9, 11, 14, 6, 4, 12, 7, 5),\n  y3 = c(7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73),\n  x4 = c(8, 8, 8, 8, 8, 8, 8, 19, 8, 8, 8),\n  y4 = c(6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89)\n)\n\nCalculamos la correlación pares de datos\n\ncor(anscombe$x1, anscombe$y1)\n\n[1] 0.8164205\n\ncor(anscombe$x2, anscombe$y2)\n\n[1] 0.8162365\n\ncor(anscombe$x3, anscombe$y3)\n\n[1] 0.8162867\n\ncor(anscombe$x4, anscombe$y4)\n\n[1] 0.8165214\n\n\nPodemos observar que los valores de las correlaciones son equivalentes, por lo tanto podríamos pensar que todos los pares de columnas se encuentran correlacionados de manera similar.\nPero, ¿será suficiente con esa información? Pasemos a revisar los gráficos de dispersión de cada par de variables.\n\nggplot(anscombe, aes(x = x1, y = y1)) +\n  geom_point(colour = \"red\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso I\")\n\n\n\nggplot(anscombe, aes(x = x2, y = y2)) +\n  geom_point(colour = \"green\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso II\")\n\n\n\nggplot(anscombe, aes(x = x3, y = y3)) +\n  geom_point(colour = \"yellow\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso III\")\n\n\n\nggplot(anscombe, aes(x = x4, y = y4)) +\n  geom_point(colour = \"orange\", \n             size = 5) +\n  geom_smooth(method = \"lm\", se = FALSE, color=\"blue\", size=0.5) +\n  labs(title = \"Caso IV\")\n\n\n\n\nComo vemos, con distintas distribuciones las correlaciones pueden ser las mismas, principalmente porque Pearson es una medida que solo captura relaciones lineales (rectas), además de verse influido fuertemente por valores extremos. Por lo mismo, es relevante siempre una buena visualización de la distribución bivariada de los datos como complemento al cálculo del coeficiente de correlación."
  },
  {
    "objectID": "assignment/04-practico.html",
    "href": "assignment/04-practico.html",
    "title": "Inferencia 1: Curva normal",
    "section": "",
    "text": "Objetivo de la práctica\nEl objetivo de esta guía práctica es introducirnos en la inferencia estadística, revisando los conceptos y aplicaciones de la curva normal y las probabilidades bajo esta con puntajes Z.\nEn detalle, aprenderemos:\n\nQué es la inferencia estadística.\nQué es la distribución normal y cómo interpretarla.\nCómo calcular probabilidades asociadas con valores Z en R.\n\n\n\n¿Qué es la inferencia estadística?\nEn estadística, llamamos inferencia al ejercicio de extrapolar determinadas estimaciones (estadístico) de una muestra a una población más grande (parámetro). En concreto, es el proceso de realizar conclusiones o predicciones sobre una población a partir de una muestra o subconjunto de esa población.\n\nUn concepto central en todo esto es la probabilidad de error, es decir, en qué medida nos estamos equivocando (o estamos dispuestos a estar equivocados) en tratar de extrapolar una estimación muestral a la población.\n\n\nDistribución normal\n\n\n\n\n\n\nNota\n\n\n\nRecordemos que por distribución nos referimos al conjunto de todos los valores posibles de una variable y las frecuencias (o probabilidades) con las que se producen.\n\n\nExisten distribuciones empíricas y distribuciones teóricas, en donde:\n\nlas primeras reflejan la distribución de los valores que asume la variable en un grupo concreto a partir de una observación.\nlas segundas son una función matématica que expresan la distribución de un conjunto de números mediante su probabilidad de ocurencia.\n\nPara empezar, veamos una de las distribuciones teóricas más conocidas: la distribución normal estándar. La distribución normal estándar:\n\nes una distribución normal con una media de 0 y una desviación estándar de 1.\nsimétricas y con un solo punto de elevación\nla media se sitúa al centro, y la desviación estandar expresa su dispersión\nla pendiente es más fuerte cerca del centro, y se suaviza hacia los extremos\nlos puntos en los que tiene lugar este cambio de curvatura se hallan a una distancia σ, a ambos lados de la media µ.\n\nCon R es posible generar un conjunto de datos simulados con una distribución normal.\n\nx.values &lt;- seq(-4,4, length = 1000)\ny.values &lt;- dnorm(x.values)\nplot(x.values, y.values, type=\"l\", xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\n¿Qué estamos haciendo en cada una de las 3 líneas de código? ¿Qué variables se crearon y cómo nos aseguramos de que los datos generados siguieran una distribución normal? Pensemos un poco…\nAhora podemos preguntar qué parte de la curva cae por debajo de un valor particular. Por ejemplo, preguntaremos sobre el valor 0 antes de ejecutar el código. Piense ¿cuál debería ser la respuesta?\n\npnorm(q = 0)\n\n[1] 0.5\n\n\nTenemos que la probabilidad (en una curva normal estándar) de obtener un valor igual o menor a 0 es de 0.5, es decir, del 50%, pero ¿por qué?\n\nPorque como la distribución normal es simétrica alrededor de cero, la probabilidad de que sea menor o igual a cero es 0.5, es decir, el 50% de la distribución está por debajo de cero y el otro 50% está por encima de cero.\n\n\n\n\n:scale 65%\n\n\nEsto es posible mediante la relación entre las áreas bajo la curva normal y las probabilidades.\n\n\nProbabilidades asociadas con valores z\n\n\n\n\n\n\nNota\n\n\n\nLa puntuación Z es una medida que se utiliza para expresar la posición relativa de un valor con respecto a la media en una distribución normal. La puntuación Z mide cuántas desviaciones estándar está un valor por encima o por debajo de la media.\n\n\nEn los ejemplos siguientes, usaremos valores Z de + 1,96 y -1,96 porque sabemos que estos valores aproximados marcan el 2,5% superior e inferior de la distribución normal estándar. Esto corresponde a un alfa típico = 0,05 para una prueba de hipótesis de dos colas (sobre la cual aprenderemos más en las próximas semanas).\n\npnorm(q = 1.96, lower.tail=TRUE)\n\n[1] 0.9750021\n\n\nLa respuesta nos dice lo que ya sabemos: el 97,5% de la distribución normal ocurre por debajo del valor z de 1,96.\nPodemos agregar una línea al gráfico para mostrar dónde se usaría abline.\nEl 97,5% de la distribución queda por debajo de esta línea.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1.96)\n\n\n\n\ninteger(0)\n\n\n¿Y si lo hacemos hacia la cola izquierda o inferior de la distribución?\n\npnorm(q = -1.96, lower.tail = TRUE)\n\n[1] 0.0249979\n\n\nTenemos que, hacia el extremo inferior de la distribución, el valor z -1,96 marca el 2,5% inferior de la distribución normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = -1.96)\n\n\n\n\ninteger(0)\n\n\n\nEjercicio 1\nUtilice la función abline() para agregar líneas en el puntaje z apropiado para demostrar el clásico 68-95-99.7 de esta curva normal estándar.\n\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\") +\nabline(v = 1) +\nabline(v = -1) +\nabline(v = 2) +\nabline(v = -2) +\nabline(v = 3) +\nabline(v = -3)\n\n\n\n\ninteger(0)\n\n\nComo se discutió en clases, también podemos hacer lo contrario: decidir primero cuánta probabilidad queremos (percentil) y luego calcular qué valores críticos están asociados con esas probabilidades. Esto utiliza la función qnorm. Si queremos saber qué valor z marca la probabilidad p del 2,5% inferior de una distribución normal estándar, usaríamos:\n\nqnorm(p = 0.025)\n\n[1] -1.959964\n\n\nEsto nos dice que el valor z de -1,96 marca el 2,5% inferior de la distribución normal estándar. Para determinar el valor z que marca el 2,5% superior de la distribución, escribo:\n\nqnorm(p = 0.975)\n\n[1] 1.959964\n\n\n\n\nEjercicio 2\nHasta ahora hemos demostrado todo con una distribución normal estándar. Pero la mayoría de las curvas normales no son normales estándar.\nGenere una curva (como hicimos anteriormente para la distribución normal estándar) y trácela con una media de 20 y una desviación estándar de 1,65.\n\nx.values &lt;- seq(10,30, length = 1000)\ny.values &lt;- dnorm(x.values, mean = 20, sd = 1.65) # indico media y sd\nplot(x.values, y.values, type=\"l\", lty=1, xlab=\"Z value\", ylab=\"Probability\", main=\"Normal Distribution\")\n\n\n\n\nAhora, identifique el valor en el que el 97,5% de la distribución cae por debajo de este valor. Esto lo hicimos antes con qnorm.\n\nqnorm(p = .975, mean = 20, sd = 1.65)\n\n[1] 23.23394\n\n\nTenemos que el 97,5% de los valores estarán por debajo de 23,2.\n\n\n\nEjercicio de aplicación\nAhora que hemos generado distribuciones normales, echemos un vistazo a algunos datos y compárelos con la distribución normal. Utilizaremos un conjunto de datos desde internet, con mediciones de 247 hombres y 260 mujeres, la mayoría de los cuales eran considerados adultos jóvenes sanos.P uede encontrar una clave para los nombres de las variables aquí, pero nos centraremos en solo tres columnas: peso en kg (wgt), altura en cm (hgt) y sexo (1 = hombre; 0 = mujer).\n\nload(url(\"http://www.openintro.org/stat/data/bdims.RData\"))\n\nSeparemos estos datos en dos conjuntos, uno de hombres y otro de mujeres con la función subset\n\nmdims &lt;- subset(bdims, sex == 1)\nfdims &lt;- subset(bdims, sex == 0)\n\n\nEjercicio 1\nHaz un histograma de la altura de los hombres y un histograma de la altura de las mujeres. ¿Cómo compararía los diversos aspectos de las dos distribuciones?\n\nhist(mdims$hgt, xlim = c(150,200))\n\n\n\nhist(fdims$hgt, xlim = c(140,190))\n\n\n\n\n\n\nEjercicio 2\nscale es una función en R y se puede aplicar a cualquier vector numérico (lista de números en R). Genere los dos histogramas siguientes, esta vez graficando scale() de las estaturas y determine cómo la versión escalada de las alturas corresponde a las alturas originales. ¿Qué calcula la escala para cada punto?\n\nhist(scale(mdims$hgt))\n\n\n\nhist(scale(fdims$hgt))\n\n\n\n\n\n\nEjercicio 3\nNos gustaría comparar la distribución de estaturas en este conjunto de datos con la distribución normal. Para cada uno de los histogramas de alturas (sin escalar), trace una curva normal en la parte superior del histograma.\n\nCalcule la media y la desviación estándar para las alturas femeninas y guárdelas como variables, fhgtmean y fhgtsd, respectivamente.\nDetermine la lista de valores de x (el rango del eje X) y guarde este vector. Puede hacer fácilmente una lista de números usando la función seq() como lo hemos hecho antes, o teniendo el límite inferior:límite superior. Por ejemplo, para generar un vector (lista de números) del 1 al 10 y guardarlo como one_ten, usaría one_ten &lt;- 1:10.\nComo arriba, use dnorm() para tomar la lista de valores de x y encontrar el valor de y correspondiente si fuera una distribución normal perfecta. Guarde este vector como la variable y.\nVuelva a trazar su histograma y luego, en la siguiente línea, use lines(x = x, y = y, col = \"blue\") para dibujar una distribución normal encima.\n\n\nfhgtmean &lt;- mean(fdims$hgt)\nfhgtsd   &lt;- sd(fdims$hgt)\nhist(fdims$hgt, probability = TRUE, ylim = c(0, .07))\nx &lt;- 140:190\ny &lt;- dnorm(x = x, mean = fhgtmean, sd = fhgtsd)\nlines(x = x, y = y, col = \"blue\")\n\n\n\n\nSegún este gráfico, ¿parece que los datos siguen una distribución casi normal? Haz lo mismo con las estaturas masculinas.\n\nRespuesta: En general, sí, consideraría que estos valores siguen una distribución casi normal ya que el histograma se ajusta bastante bien a la curva.\n\nObserve que la forma del histograma es una forma de determinar si los datos parecen estar distribuidos casi normalmente, pero puede resultar frustrante decidir qué tan cerca está el histograma de la curva. Un enfoque alternativo implica construir una gráfica de probabilidad normal, también llamada gráfica Q-Q por “quantil-quantil”. Ejecute ambas líneas juntas.\n\nqqnorm(fdims$hgt)\nqqline(fdims$hgt)\n\n\n\n\nUn QQ plot nos muestra en el eje x los cuantiles teóricos de la distribución en términos de desviaciones estandar, y en el eje y los valores de la variable. La distribución de los puntos en una línea recta es una indicación de que los datos se distribuyen normalmente.\nVeamos otro ejemplo de otra variable de la base de datos:\n\nhist(fdims$che.de)\n\n\n\nqqnorm(fdims$che.de)\nqqline(fdims$che.de)\n\n\n\n\nUna vez que decidimos que una variable se distribuyte de forma normal, podemos responder todo tipo de preguntas sobre esa variable relacionadas con la probabilidad. Tomemos, por ejemplo, la pregunta: “¿Cuál es la probabilidad de que una mujer adulta joven elegida al azar mida más 182 cm?”\nSi suponemos que las alturas de las mujeres se distribuyen normalmente (una aproximación muy cercana también está bien), podemos encontrar esta probabilidad calculando una puntuación Z y consultando una tabla Z (también llamada tabla de probabilidad normal).\nEn R, esto se hace en un solo paso con la función pnorm (como hicimos anteriormente para la distribución normal estándar).\n\npnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.9955656\n\n\nObtenemos la proporción de mujeres que está bajo esa estatura, es decir 99,6%. Si queremos saber la proporción de mujeres que está sobre esa estatura:\n\n1 - pnorm(q = 182, mean = fhgtmean, sd = fhgtsd)\n\n[1] 0.004434387\n\n\nEn este caso, el 0,4% de las mujeres se encontraría sobre esa estatura.\nPodemos también hacer la operación inversa, es decir, a qué valor (estatura) corresponde un porcentaje o probabilidad basada en una distribución normal. Para ello utilizamos la función qnorm. Por ejemplo, para la probabilidad que calculamos más arriba para una altura de 182cm en las mujeres:\n\nqnorm(.9955656, fhgtmean, fhgtsd)\n\n[1] 182\n\n\n\n\n\nReporte de progreso\nCompletar el reporte de progreso correspondiente a esta práctica aquí. El plazo para contestarlo es hasta el día viernes de la semana en la que se publica la práctica correspondiente.\n\n\nVideo de práctica\n\n\n\nForo"
  },
  {
    "objectID": "assignment/index.html",
    "href": "assignment/index.html",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos. La organización semestras se puede revisar en la planificación del curso.\nEl trabajo con estas guías se organiza en los siguientes momentos:\n\nla guía se presenta los días martes y se avanza en resolver dudas para su resolución\ncada estudiante realiza la guía de manera autónoma durante la semana\nen caso de dudas que surjan durante la semana se recomienda preguntar en el foro de U-Cursos disponible para este fin\nlas sesiones prácticas se realizarán por lo general en grupos pequeños guiados por ayudantes.\ncada semana se completa un reporte de progreso (detalles abajo)\n\nEn las prácticas vamos a trabajar con el software R, Versión 4.3.1."
  },
  {
    "objectID": "assignment/index.html#descripción",
    "href": "assignment/index.html#descripción",
    "title": "Instrucciones generales",
    "section": "",
    "text": "Los prácticos consisten en el desarrollo de una guía práctica (por lo general cada semana) donde se aplican y profundizan los contenidos de las clases, y donde también se abordan otras temáticas relacionadas al manejo y repote de datos. La organización semestras se puede revisar en la planificación del curso.\nEl trabajo con estas guías se organiza en los siguientes momentos:\n\nla guía se presenta los días martes y se avanza en resolver dudas para su resolución\ncada estudiante realiza la guía de manera autónoma durante la semana\nen caso de dudas que surjan durante la semana se recomienda preguntar en el foro de U-Cursos disponible para este fin\nlas sesiones prácticas se realizarán por lo general en grupos pequeños guiados por ayudantes.\ncada semana se completa un reporte de progreso (detalles abajo)\n\nEn las prácticas vamos a trabajar con el software R, Versión 4.3.1."
  },
  {
    "objectID": "assignment/index.html#reporte",
    "href": "assignment/index.html#reporte",
    "title": "Instrucciones generales",
    "section": "Reportes de progreso",
    "text": "Reportes de progreso\nEste curso se caracteriza por el desarrollo secuencial y acumulativo de aprendizajes. En otras palabras, va a ser muy difícil poder lograr los objetivos de aprendizaje posteriores sin haber logrado los objetivos de contenidos previos (es muy difícil aprender a multiplicar sin saber sumar). Por lo tanto, como equipo a cargo del curso nos interesa poder monitorear permanentemente el cumplimiento de objetivos de aprendizaje semana a semana para así poder prestar asesoría oportuna.\nEl sistema de monitoreo permanente de cumplimiento de objetivos se llevará a cabo mediante reportes de progreso. Los reportes consisten en completar un formulario simple y breve, donde se preguntará por el cumplimiento de los objetivos de las prácticas respectivas. El link para completar el reporte se encuentra al final de cada guía Se deben completar durante la semana en que se desarrolla la guía, a más tardar los días viernes.\nComo incentivo para completar los reportes de progreso, se entregarán dos décimas adicionales a la nota de quienes tengan al menos el 80% de los reportes de progreso completados entre instancias de evaluación (por ejemplo, en el caso de la evaluación 2 se deben tener todos los reportes de progreso completados entre la evaluación 1 y 2)."
  },
  {
    "objectID": "assignment/index.html#trabajo-con-software-r",
    "href": "assignment/index.html#trabajo-con-software-r",
    "title": "Instrucciones generales",
    "section": "Trabajo con software R",
    "text": "Trabajo con software R\nPara los análisis estadísticos de este curso usamos el programa R, en parte porque es gratuito, pero la principal razón es que es de código abierto. Esto quiere decir que cualquier persona puede revisar cómo está hecho y aportar con modificaciones y procedimientos nuevos, como son las librerías que realizan funciones específicas.\nEl carácter de apertura de R posee muchas ventajas, pero también conlleva complicaciones. Se actualiza permanentemente, así como también las librerías, y esto puede generar problemas de compatibilidad y de fallas en ejecución del código de análisis.\nPara minimizar estos posibles problemas en este curso, vamos a:\n\ntrabajar con la misma y última versión de R, que es la 4.3\nevitar uso de tilde, ñ, espacios y mayúsculas tanto en carpetas y archivos, así como también en los nombres de las variables"
  },
  {
    "objectID": "assignment/index.html#sobre-errores-y-consultas-sobre-problemas-con-r-y-ejecución-de-código",
    "href": "assignment/index.html#sobre-errores-y-consultas-sobre-problemas-con-r-y-ejecución-de-código",
    "title": "Instrucciones generales",
    "section": "Sobre errores y consultas sobre problemas con R y ejecución de código",
    "text": "Sobre errores y consultas sobre problemas con R y ejecución de código\nEn caso de problemas con ejecución de código, se sugiere intentar solucionarlo autónomamente por no más de 10 minutos, si los problemas siguen entonces consultar.\nSe sugiere que las consultas sobre problemas en la ejecución del código y otros se realicen en los foros al final de los prácticos correspondientes, para lo cual se requiere solo habilitar una cuenta en Github. Al hacer la consulta, adjuntar la siguiente información:\n\nCódigo completo hasta que se produce el problema\nIndicar línea del código donde se produce el problema\nAdjuntar el resultado del output de la información de la sesión (sessionInfo())\n\n\nInstalación de R & RStudio\nPara esta versión del curso vamos a trabajar con el programa R Version 4.3 y con RStudio, que ofrece un entorno más amigable para trabajar con R.\nPara instalar R: ir a https://cran.r-project.org/index.html y bajar/instalar la versión correspondiente a la plataforma utilizada (Windows, Mac o Linux)\nPara instalar RStudio: ir a https://rstudio.com/products/rstudio/ y bajar/instalar RStudio desktop, Open Source License (libre).\nEn caso de dudas se puede revisar el siguiente video tutorial de instalación de R & RStudio, preparado por Julio Iturra (apoyo docente) del curso Estadística Multivariada 2020:\n\n\n\n\n\nSi por alguna razón se prefiere trabajar sin descargar, también se puede utilizar RCloud, abajo un tutorial preparado por Valentina Andrade para el curso de Estadística Multivariada:\n\n\n\n\n\n\n\nSobre el trabajo en hojas de código en RStudio\n\nEl trabajo de análisis en RStudio se efectua en una hoja de código (o R script o sintaxis, o para los usuarios de Stata la do-file), que es donde se anotan los comandos y funciones. Para abrir una hoja, en RStudio ir a File &gt; New File &gt; R Script (o ctrl+shift+N),y aparecerá un panel con una pestaña “Untitled” (sin título). Esta es la hoja de código donde se anotan los comandos.\nLos contenidos de las hojas de código son básicamente 2:\n\ncomandos o funciones: se escriben en la hoja, y para ejecutarlos se debe posicionar el cursor en la línea respectiva y ctrl+enter, el resultado aparecerá en el panel de resultados o Consola.\ntexto: para escribir títulos, comentarios, y todo lo que permita entender qué se está haciendo, al principio de la línea respectiva escribir el signo #\n\nPara grabar nuestra hoja de código y así respaldar nuestros análisis, File &gt; Save (o ctrl+s), y dar un nombre al archivo. Recordar: breve, sin espacios ni tildes ni eñes. Por defecto, la extensión de estos archivos es .R"
  },
  {
    "objectID": "content/02-content.html",
    "href": "content/02-content.html",
    "title": "Asociación 1: Bases",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "content/04-content.html",
    "href": "content/04-content.html",
    "title": "Asociación 3: Tamaño de efecto, otros coeficientes de correlación y matrices",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "content/06-content.html",
    "href": "content/06-content.html",
    "title": "Inferencia 2 - Error e intervalos de confianza",
    "section": "",
    "text": "Documento de presentación\n\n\n\n\nForo"
  },
  {
    "objectID": "example/cace.html",
    "href": "example/cace.html",
    "title": "Complier average treatment effects",
    "section": "",
    "text": "Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population. There are all sorts of CATEs: you can find the CATE for men vs. women, for people who are treated with the program (the average treatment on the treated, or ATT or TOT), for people who are not treated with the program (the average treatment on the untreated, or ATU), and so on.\nOne important type of CATE is the effect of a program on just those who comply with the program. We can call this the complier average treatment effect, but the acronym would be the same as conditional average treatment effect, so we’ll call it the complier average causal effect or CACE.\nThinking about compliance is important. You might randomly assign people to receive treatment or a program, but people might not do what you tell them. Additionally, people might do the program if assigned to do it, but they would have done it anyway. We can split the population into four types of people:\n\nCompliers: People who follow whatever their assignment is (if assigned to treatment, they do the program; if assigned to control, they don’t)\nAlways takers: People who will receive or seek out the program regardless of assignment (if assigned to treatment, they do the program; if assigned to control, they still do the program)\nNever takers: People who will not receive or seek out the program regardless of assignment (if assigned to treatment, they don’t do the program; if assigned to control, they also don’t do it)\nDefiers: People who will do the opposite of whatever their assignment is (if assigned to treatment, they don’t do the program; if assigned to control, they do the program)\n\nTo simplify things, evaluators and econometricians assume that defiers don’t exist based on the idea of monotonicity, which means that we can assume that the effect of being assigned to treatment only increases the likelihood of participating in the program (and doesn’t make it more likely).\nThe tricky part about trying to find who the compliers are in a sample is that we can’t know what people would have done in the absence of treatment. If we see that someone in the experiment was assigned to be in the treatment group and they then participated in the program, they could be a complier (since they did what they were assigned to do), or they could be an always taker (they did what they were assigned to do, but they would have done it anyway). Due to the fundamental problem of causal inference, we cannot know what each person would have done in a parallel world.\nWe can use data from a hypothetical program to see how these three types of compliers distort our outcomes, and more importantly, how we can disentangle compliers from their always- and never-taker counterparts.\nIf you want to follow along with this example, you can download these two datasets:\n\n bed_nets_time_machine.csv\n bed_nets_observed.csv"
  },
  {
    "objectID": "example/cace.html#compliance-and-treatment-effects",
    "href": "example/cace.html#compliance-and-treatment-effects",
    "title": "Complier average treatment effects",
    "section": "",
    "text": "Throughout this course, we’ve talked about the difference between the average treatment effect (ATE), or the average effect of a program for an entire population, and conditional average treatment effect (CATE), or the average effect of a program for some segment of the population. There are all sorts of CATEs: you can find the CATE for men vs. women, for people who are treated with the program (the average treatment on the treated, or ATT or TOT), for people who are not treated with the program (the average treatment on the untreated, or ATU), and so on.\nOne important type of CATE is the effect of a program on just those who comply with the program. We can call this the complier average treatment effect, but the acronym would be the same as conditional average treatment effect, so we’ll call it the complier average causal effect or CACE.\nThinking about compliance is important. You might randomly assign people to receive treatment or a program, but people might not do what you tell them. Additionally, people might do the program if assigned to do it, but they would have done it anyway. We can split the population into four types of people:\n\nCompliers: People who follow whatever their assignment is (if assigned to treatment, they do the program; if assigned to control, they don’t)\nAlways takers: People who will receive or seek out the program regardless of assignment (if assigned to treatment, they do the program; if assigned to control, they still do the program)\nNever takers: People who will not receive or seek out the program regardless of assignment (if assigned to treatment, they don’t do the program; if assigned to control, they also don’t do it)\nDefiers: People who will do the opposite of whatever their assignment is (if assigned to treatment, they don’t do the program; if assigned to control, they do the program)\n\nTo simplify things, evaluators and econometricians assume that defiers don’t exist based on the idea of monotonicity, which means that we can assume that the effect of being assigned to treatment only increases the likelihood of participating in the program (and doesn’t make it more likely).\nThe tricky part about trying to find who the compliers are in a sample is that we can’t know what people would have done in the absence of treatment. If we see that someone in the experiment was assigned to be in the treatment group and they then participated in the program, they could be a complier (since they did what they were assigned to do), or they could be an always taker (they did what they were assigned to do, but they would have done it anyway). Due to the fundamental problem of causal inference, we cannot know what each person would have done in a parallel world.\nWe can use data from a hypothetical program to see how these three types of compliers distort our outcomes, and more importantly, how we can disentangle compliers from their always- and never-taker counterparts.\nIf you want to follow along with this example, you can download these two datasets:\n\n bed_nets_time_machine.csv\n bed_nets_observed.csv"
  },
  {
    "objectID": "example/cace.html#finding-compliers-with-a-mind-reading-time-machine",
    "href": "example/cace.html#finding-compliers-with-a-mind-reading-time-machine",
    "title": "Complier average treatment effects",
    "section": "Finding compliers with a mind-reading time machine",
    "text": "Finding compliers with a mind-reading time machine\nFirst let’s load the data and reorder some of the categories:\n\n\nCode\nlibrary(tidyverse)  # ggplot(), %&gt;%, mutate(), and friends\nlibrary(broom)  # Convert models to data frames\nlibrary(estimatr)  # Run 2SLS models in one step with iv_robust()\n\nbed_nets &lt;- read_csv(\"data/bed_nets_observed.csv\") %&gt;%\n  # Make \"No bed net\" (control) come first\n  mutate(bed_net = fct_relevel(bed_net, \"No bed net\"))\n\nbed_nets_time_machine &lt;- read_csv(\"data/bed_nets_time_machine.csv\") %&gt;%\n  # Make \"No bed net\" come first and \"Complier\" come first\n  mutate(bed_net = fct_relevel(bed_net, \"No bed net\"),\n         status = fct_relevel(status, \"Complier\"))\n\n\nThis is what we would be able to see if we could read everyone’s minds. There are always takers who will use a bed net regardless of the program, and they’ll have higher health outcomes. However, those better outcomes are because of something endogenous—there’s something else that makes these people always pursue bed nets, and that’s likely related to health. We probably want to not consider them when looking for the program effect. There are never takers who won’t ever use a bed net, and they have worse health outcomes. Again, there’s endogeneity here—something is causing them to not use the bed nets, and it likely also causes their health level. We don’t want to look at them either.\nThe first group—the compliers—are the people we want to focus on. Here we see that the program had an effect when compared to a control group.\n\n\nCode\nset.seed(1234)  # Make the jittering the same every time\n\nggplot(bed_nets_time_machine, aes(y = health, x = treatment)) +\n  geom_point(aes(shape = bed_net, color = status),\n             position = position_jitter(height = NULL, width = 0.25)) +\n  facet_wrap(vars(status)) +\n  labs(color = \"Type of person\", shape = \"Compliance\",\n       x = NULL, y = \"Health status\") +\n  scale_color_viridis_d(option = \"plasma\", end = 0.85) +\n  theme_bw()"
  },
  {
    "objectID": "example/cace.html#finding-compliers-in-actual-data",
    "href": "example/cace.html#finding-compliers-in-actual-data",
    "title": "Complier average treatment effects",
    "section": "Finding compliers in actual data",
    "text": "Finding compliers in actual data\nThis is what we actually see in the data, though. You can tell who some of the always takers are (those who used bed nets after being assigned to the control group) and who some of the never takers are (those who did not use a bed net after being assigned to the treatment group), but compliers are mixed up with the always and never takers. We have to somehow disentangle them!\n\n\nCode\nset.seed(1234)\nggplot(bed_nets_time_machine, aes(y = health, x = bed_net)) +\n  geom_point(aes(shape = bed_net, color = status),\n             position = position_jitter(height = NULL, width = 0.25)) +\n  facet_wrap(vars(treatment)) +\n  labs(color = \"Type of person\", shape = \"Compliance\",\n       x = NULL, y = \"Health status\") +\n  scale_color_viridis_d(option = \"plasma\", end = 0.85) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nWe can do this by assuming the proportion of compliers, never takers, and always takers are equally spread across treatment and control (which we can assume through the magic of randomization). If that’s the case, we can calculate the intent to treat (ITT) effect, which is the CATE of being assigned treatment (or the effect of being assigned treatment on health status, regardless of actual compliance).\nThe ITT is actually composed of three different causal effects: the complier average causal effect (CACE), the always taker average causal effect (ATACE), and the never taker average causal effect (NTACE). In the formula below, \\(\\pi\\) stands for the proportion of people in each group. Formally, the ITT can be defined like this:\n\\[\n\\begin{aligned}\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{compliers} \\times (\\text{T} - \\text{C})_\\text{compliers}} + \\\\\n&\\color{#B7318A}{\\pi_\\text{always takers} \\times (\\text{T} - \\text{C})_\\text{always takers}} + \\\\\n&\\color{#FEBA2C}{\\pi_\\text{never takers} \\times (\\text{T} - \\text{C})_\\text{never takers}}\n\\end{aligned}\n\\]\nWe can simplify this to this acronymized version:\n\\[\n\\text{ITT}\\ =\\ \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}}\n\\]\nThe number we care about the most here is the CACE, which is stuck in the middle of the equation. But we can rescue it with some fun logical and algebraic trickery!\nIf we assume that assignment to treatment doesn’t make someone more likely to be an always taker or a never taker, we can set the ATACE and NTACE to zero, leaving us with just three variables to worry about: ITT, \\(\\pi_\\text{c}\\), and CACE:\n\\[\n\\begin{aligned}\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\text{ATACE}} + \\color{#FEBA2C}{\\pi_\\text{N} \\text{NTACE}} \\\\[6pt]\n=\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}} + \\color{#B7318A}{\\pi_\\text{A} \\times 0} + \\color{#FEBA2C}{\\pi_\\text{N} \\times 0}\\\\[6pt]\n\\text{ITT}\\ =\\ & \\color{#0D0887}{\\pi_\\text{C} \\text{CACE}}\n\\end{aligned}\n\\]\nWe can use algebra to rearrange this formula so that we’re left with an equation that starts with CACE (since that’s the value we care about):\n\\[\n\\text{CACE} = \\frac{\\text{ITT}}{\\pi_\\text{C}}\n\\]\nIf we can find the ITT and the proportion of compliers, we can find the complier average causal effect (CACE). Fortunately, both those pieces—ITT and \\(\\pi_\\text{C}\\)—are findable in the data we have!"
  },
  {
    "objectID": "example/cace.html#finding-the-itt",
    "href": "example/cace.html#finding-the-itt",
    "title": "Complier average treatment effects",
    "section": "Finding the ITT",
    "text": "Finding the ITT\nThe ITT is easy to find with a simple OLS model:\n\n\nCode\nitt_model &lt;- lm(health ~ treatment, data = bed_nets)\n\ntidy(itt_model)\n## # A tibble: 2 × 5\n##   term               estimate std.error statistic  p.value\n##   &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n## 1 (Intercept)           40.9      0.444     92.1  0       \n## 2 treatmentTreatment     5.99     0.630      9.51 5.20e-21\n\nITT &lt;- tidy(itt_model) %&gt;%\n  filter(term == \"treatmentTreatment\") %&gt;%\n  pull(estimate)\n\n\nThe ITT here is ≈6—being assigned treatment increases average health status by 5.99 health points."
  },
  {
    "objectID": "example/cace.html#finding-the-proportion-of-compliers",
    "href": "example/cace.html#finding-the-proportion-of-compliers",
    "title": "Complier average treatment effects",
    "section": "Finding the proportion of compliers",
    "text": "Finding the proportion of compliers\nThe proportion of compliers is a little trickier, but doable with some algebraic trickery. Recall from the graph above that the people who were in the treatment group and who complied are a combination of always takers and compliers. This means we can say:\n\\[\n\\begin{aligned}\n\\pi_\\text{A} + \\pi_\\text{C} =& \\text{% yes in treatment; or} \\\\\n\\pi_\\text{C} =& \\text{% yes in treatment} - \\pi_\\text{A}\n\\end{aligned}\n\\]\nWe actually know \\(\\pi_\\text{A}\\)—remember in the graph above that the people who were in the control group and who used bed nets are guaranteed to be always takers (none of them are compliers or never takers). If we assume that the proportion of always takers is the same in both treatment and control, we can use that percent here, giving us this final equation for \\(\\pi_\\text{C}\\):\n\\[\n\\begin{aligned}\n\\pi_\\text{C} =& \\text{% yes in treatment} - \\pi_\\text{A} \\\\\n=& \\text{% yes in treatment} - \\text{% yes in control}\n\\end{aligned}\n\\]\nSo, if we can find the percent of people assigned to treatment who used bed nets, find the percent of people assigned to control and used bed nets, and subtract the two percentages, we’ll have the proportion of compliers, or \\(\\pi_\\text{C}\\). We can do that with the data we have (61% - 19.5% = 41.5% compliers):\n\n\nCode\nbed_nets %&gt;%\n  group_by(treatment, bed_net) %&gt;%\n  summarize(n = n()) %&gt;%\n  mutate(prop = n / sum(n))\n## # A tibble: 4 × 4\n## # Groups:   treatment [2]\n##   treatment bed_net        n  prop\n##   &lt;chr&gt;     &lt;fct&gt;      &lt;int&gt; &lt;dbl&gt;\n## 1 Control   No bed net   808 0.805\n## 2 Control   Bed net      196 0.195\n## 3 Treatment No bed net   388 0.390\n## 4 Treatment Bed net      608 0.610\n\n# pi_c = prop yes in treatment - prop yes in control\npi_c &lt;- 0.6104418 - 0.1952191\n\n\nFinally, now that we know both the ITT and \\(\\pi_\\text{C}\\), we can find the CACE (or the LATE):\n\n\nCode\nCACE &lt;- ITT / pi_c\nCACE\n## [1] 14.43\n\n\nIt’s 14.4, which means that using bed nets increased health by 14 health points for compliers (which is a lot bigger than the 6 that we found before). We successfully filtered out the always takers and the never takers, and we have our complier-specific causal effect."
  },
  {
    "objectID": "example/cace.html#finding-the-cacelate-with-iv2sls",
    "href": "example/cace.html#finding-the-cacelate-with-iv2sls",
    "title": "Complier average treatment effects",
    "section": "Finding the CACE/LATE with IV/2SLS",
    "text": "Finding the CACE/LATE with IV/2SLS\nDoing that is super tedious though! What if there was an easier way to find the effect of the bed net program for just the compliers? We can do this with IV/2SLS regression by using assignment to treatment as an instrument.\nAssignment to treatment works as an instrument because it’s (1) relevant, since being told to use bed nets is probably highly correlated with using bed nets, (2) exclusive, since the only way that being told to use bed nets can cause changes in health is through the actual use of the bed nets, and (3) exogenous, since being told to use bed nets probably isn’t related to other things that cause health.\nHere’s a 2SLS regression with assignment to treatment as the instrument:\n\n\nCode\nmodel_2sls &lt;- iv_robust(health ~ bed_net | treatment, data = bed_nets)\ntidy(model_2sls)\n##             term estimate std.error statistic   p.value conf.low conf.high   df outcome\n## 1    (Intercept)    38.12    0.5151     74.01 0.000e+00    37.11     39.13 1998  health\n## 2 bed_netBed net    14.43    1.2538     11.51 1.038e-29    11.97     16.89 1998  health\n\n\nThe coefficient for bed_net is identical to the CACE that we found manually! Instrumental variables are helpful for isolated program effects to only compliers when you’re dealing with noncompliance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "\n            Estadística Correlacional\n        ",
    "section": "",
    "text": "Estadística Correlacional\n        \n        \n            Asociación, inferencia y reporte\n        \n        \n            SOC01019 • Segundo Semestre 2023Departamento de Sociología, Facultad de Ciencias SocialesUniversidad de Chile\n        \n    \n    \n      \n        \n        \n        \n      \n    \n\n\n\n\n\nEquipo docente\nProfesor\n\n\n\nProf. Juan Carlos Castillo\n\n   325 Sociología FACSO, Universidad de Chile\n   juancastillov@uchile.cl\n   Agendar reunión\n\nApoyos docentes\n\n\n\nDaniela Olivares\n\n   danielaolivarescollio@gmail.com\n   Agendar reunión\n\n\n\n\nAndreas Laffert\n\n   andreas.laffert@ug.uchile.cl\n   Agendar reunión\n\nAyudantes\n\nBladimir Beltran\nMatías Buller\nNeyem Cares\nCarolina Collao\nAníbal Figueroa\nValentina Frómeta\nPedro González\nAnaís Herrada\nMoira Martínez\nEmilio Miranda\n\n\n\nActividades\n\n   Lunes 8:30-10:00 - Aulario A/7-8\n   Martes 10:15-11:45 Online / Laboratorio computación 2\n\n\n\nContacto\nE-mail es la mejor manera de ponerse en contacto conmigo y con el equipo docente del curso."
  },
  {
    "objectID": "news/2023-08-14_infos.html",
    "href": "news/2023-08-14_infos.html",
    "title": "Informaciones de la semana",
    "section": "",
    "text": "← News\n\n\n\nActualizaciones sitio web:"
  },
  {
    "objectID": "resource/index.html",
    "href": "resource/index.html",
    "title": "Recursos",
    "section": "",
    "text": "En esta sección se irán subiendo una serie de recursos relacionados con el curso."
  },
  {
    "objectID": "resource/index.html#glosario-de-conceptos",
    "href": "resource/index.html#glosario-de-conceptos",
    "title": "Recursos",
    "section": "Glosario de conceptos",
    "text": "Glosario de conceptos\n\n\n\nConcepto\nDefinición\n\n\n\n\nEstadística \nConjunto de métodos y herramientas que involucra la recopilación, análisis, interpretación y presentación de datos numéricos con el objetivo de describir patrones, relaciones y tendencias en fenómenos naturales o sociales.\n\n\nReproducibilidad \nLa capacidad de regenerar un experimento, análisis o estudio utilizando los mismos datos y métodos para llegar a los mismos resultados originales, verificando y asegurando la validez de los hallazgos.\n\n\nCiencia Social Abierta \nUn enfoque en la investigación social que promueve la transparencia, el acceso abierto a datos, métodos y resultados, y la colaboración entre investigadores para mejorar la calidad y confiabilidad de la investigación.\n\n\nProtocolo IPO (Input-Process-Output) \nSistema digital de carpetas interconectadas: entrada, proceso y salida. Se utiliza para organizar, procesar y documentar los datos y código de un proyecto de investigación para que cualquier persona pueda ejecutarlo y compartirlo.\n\n\nR project \nCarpeta raíz organizada donde trabajas en un proyecto concreto en el lenguaje de programación R, permitiéndote gestionar archivos, paquetes y configuraciones de manera específica para ese proyecto.\n\n\nTexto plano \nTipo de texto sin formato especial que se puede leer independiente del lector que se utilice.\n\n\nMarkdown \nClase especial de lenguaje que permite darle formato a texto simple con pocas marcas. Se utiliza comúnmente para escribir documentos simples con formato, como páginas web, documentación y presentaciones.\n\n\nDocumentos dinámicos \nArchivos que combinan texto plano y código de análisis (gráficos, tablas y resultados), de manera simultánea en un solo documento, permitiendo la generación automática y reproducible de resultados actualizados a medida que cambian los datos o parámetros.\n\n\nRMarkdown \nUna extensión de Markdown en el entorno R que permite la integración simultánea de texto plano y código R y su ejecución en el documento, lo que facilita la creación de documentos dinámicos con análisis estadísticos y visualizaciones.\n\n\nLibrerías \nConjuntos de funciones y herramientas predefinidas que se pueden utilizar en lenguajes de programación, como R, para realizar tareas específicas sin tener que escribir todo el código desde cero.\n\n\nKnitear \nProceso de compilación secuencial de código y resultados de ejecución en un documento RMarkdown, generando un documento final con texto formateado, código y gráficos integrados.\n\n\nRenderizar \nEn el contexto de RMarkdown se refiere al proceso de convertir el código y contenido en un documento legible y presentable. En otras palabras, cuando renderizas un documento RMarkdown, estás transformando el código, texto y elementos visuales en un formato final, como un informe, una presentación o un documento HTML, que pueda ser compartido o presentado a otros de manera comprensible.\n\n\nYAML \nAcrónimo de “YAML Ain’t Markup Language”, es un formato de serialización de datos legible por humanos que se utiliza para configurar y definir la estructura de datos en muchos programas y aplicaciones. En RMarkdown corresponden al encabezado de instrucciones generales del documento.\n\n\nChunk \nUn bloque de código, que puede ser en R, en un documento RMarkdown, rodeado por marcas especiales que indican al sistema cómo manejar y ejecutar ese fragmento de código, y luego mostrar sus resultados en el documento final."
  },
  {
    "objectID": "resource/index.html#reporte",
    "href": "resource/index.html#reporte",
    "title": "Recursos",
    "section": "Reporte",
    "text": "Reporte\n\nTablas con R, con ejemplo de canciones de Spotify\nIntroduction to Quarto"
  },
  {
    "objectID": "resource/index.html#estadística-descriptiva",
    "href": "resource/index.html#estadística-descriptiva",
    "title": "Recursos",
    "section": "Estadística descriptiva",
    "text": "Estadística descriptiva\n\nCurso Estadística Descriptiva Sociología UChile, 1er Sem 2023"
  },
  {
    "objectID": "resource/index.html#uso-de-r",
    "href": "resource/index.html#uso-de-r",
    "title": "Recursos",
    "section": "Uso de R",
    "text": "Uso de R\n\nConocimientos básicos de programación en R\nImportar datos en R\nProcesamiento y análisis de datos en R (tidyverse)\nProcesamiento y análisis de datos en R (base)\nMás para aprender R"
  },
  {
    "objectID": "resource/index.html#inferencia",
    "href": "resource/index.html#inferencia",
    "title": "Recursos",
    "section": "Inferencia",
    "text": "Inferencia\n\nStatistical Inference via Data Science A ModernDive into R and the Tidyverse\nIntroduction to modern statistics (Mine Çetinkaya-Rundel and Johanna Hardin)\nInferencia univariada"
  },
  {
    "objectID": "resource/index.html#visualización",
    "href": "resource/index.html#visualización",
    "title": "Recursos",
    "section": "Visualización",
    "text": "Visualización\n\nVisualización descriptiva de datos en R\nR Graph Gallery"
  },
  {
    "objectID": "resource/index.html#bases-de-datos",
    "href": "resource/index.html#bases-de-datos",
    "title": "Recursos",
    "section": "Bases de datos",
    "text": "Bases de datos\n\nBases de datos para trabajos o investigación"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Planificación",
    "section": "",
    "text": "Este curso se compone de tres actividades principales a la semana:\n\n Clases lectivas presenciales: los lunes de 8:30 a 10:00, donde en base a las lecturas correspondientes a esa semana se presentará un resumen de los contenidos principales y se resolverán dudas.\n Talleres prácticos online: los martes de 10:15 a 11:45 se desarrollarán actividades con énfasis en el manejo de software para análisis y reporte de los resultados. Estas instancias estarán guiadas por los apoyos docentes del curso.\n Lecturas: cada sesión lectiva tiene una lectura obligatoria asignada a la semana, la que se encuentra a disposición en esta página.\n\nLas actividades semanales se resumen en el siguiente esquema:\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nLunes 07\n1. Presentación: Asociación, inferencia y reporte\n\nLeer detalladamente programa del curso\n\n\nMartes 08\n\nReportes dinámicos 1\n\n\n\nLunes 14\nAsociación 1: Bases\n\n*Richtey 1-21 : La imaginación estadística\n\n\nMartes 15\n\nFERIADO\n\n\n\nLunes 21\nAsociación 2: Pearson\n\n*Moore 97-131 Análisis de relaciones\nPiovani -The historical construction of correlation\n\n\nMartes 22\n\nCálculo y reporte correlación\n\n\n\nLunes 28\nAsociación 3: Matrices y tamaños de efecto\n\nField 205-244 Correlation\n\n\nMartes 29\n\nMatrices y tamaños de efecto\n\n\n\n Septiembre \n\n\n\n\n\nLunes 4\nInferencia 1\n\nPardo cap 6 Distribuciones muestrales\nRichtey cap 7\n\n\nMartes 5\n\nInferencia 1\n\n\n\n\n\n\n\n\n\n\n\n\n Clases\n Prácticos\n Lecturas\n\n\n\n\n Septiembre \n\n\n\n\n\nLunes 11\nPausa\n\n\n\n\nMartes 12\nPausa\n\n\n\n\nLunes 18\nFeriado\n\n\n\n\nMartes 19\nFeriado\n\n\n\n\nLunes 25\nPrueba 1\n\n\n\n\nMartes 26\n\nReporte 2\n\n\n\n Octubre \n\n\n\n\n\nLunes 2\nInferencia 2\n\nMoore cap 6\n\n\nMartes 3\n\nInferencia 2\n\n\n\nLunes 9\nFeriado\n\n\n\n\nMartes 10\n\nInstrucciones trabajo final\n\n\n\nLunes 16\nInferencia 3\n\nMoore cap 7\n\n\nMartes 17\n\nInferencia 3\n\n\n\nLunes 23\nInferencia 4\n\nMoore cap 8\n\n\nMartes 24\n\nInferencia 4\n\n\n\nLunes 30\nSemana trabajo autónomo\n\n\n\n\nMartes 31\n\nSemana trabajo autónomo\n\n\n\n\n\n\n\n\n\n\n\n\n Clases\n Prácticos\n Lecturas\n\n\n\n\n Noviembre \n\n\n\n\n\nLunes 6\nEvaluación 2 (30%)\n\n\n\n\nMartes 7\n\nReporte 3\n\n\n\nLunes 13\nAsociación con categóricas 1\n\nMoore cap 9\n\n\nMartes 14\n\nAsesoría final trabajos\n\n\n\nLunes 20\nAsociación con categóricas 2\n\nRichtey cap 13\n\n\nLunes 27\nEntrega trabajo final\nPruebas recuperativas"
  },
  {
    "objectID": "schedule.html#forma-general-de-funcionamiento",
    "href": "schedule.html#forma-general-de-funcionamiento",
    "title": "Planificación",
    "section": "",
    "text": "Este curso se compone de tres actividades principales a la semana:\n\n Clases lectivas presenciales: los lunes de 8:30 a 10:00, donde en base a las lecturas correspondientes a esa semana se presentará un resumen de los contenidos principales y se resolverán dudas.\n Talleres prácticos online: los martes de 10:15 a 11:45 se desarrollarán actividades con énfasis en el manejo de software para análisis y reporte de los resultados. Estas instancias estarán guiadas por los apoyos docentes del curso.\n Lecturas: cada sesión lectiva tiene una lectura obligatoria asignada a la semana, la que se encuentra a disposición en esta página.\n\nLas actividades semanales se resumen en el siguiente esquema:\n\n\n\n\n\n\n\n Agosto \n Clases\n Prácticos\n Lecturas\n\n\nLunes 07\n1. Presentación: Asociación, inferencia y reporte\n\nLeer detalladamente programa del curso\n\n\nMartes 08\n\nReportes dinámicos 1\n\n\n\nLunes 14\nAsociación 1: Bases\n\n*Richtey 1-21 : La imaginación estadística\n\n\nMartes 15\n\nFERIADO\n\n\n\nLunes 21\nAsociación 2: Pearson\n\n*Moore 97-131 Análisis de relaciones\nPiovani -The historical construction of correlation\n\n\nMartes 22\n\nCálculo y reporte correlación\n\n\n\nLunes 28\nAsociación 3: Matrices y tamaños de efecto\n\nField 205-244 Correlation\n\n\nMartes 29\n\nMatrices y tamaños de efecto\n\n\n\n Septiembre \n\n\n\n\n\nLunes 4\nInferencia 1\n\nPardo cap 6 Distribuciones muestrales\nRichtey cap 7\n\n\nMartes 5\n\nInferencia 1\n\n\n\n\n\n\n\n\n\n\n\n\n Clases\n Prácticos\n Lecturas\n\n\n\n\n Septiembre \n\n\n\n\n\nLunes 11\nPausa\n\n\n\n\nMartes 12\nPausa\n\n\n\n\nLunes 18\nFeriado\n\n\n\n\nMartes 19\nFeriado\n\n\n\n\nLunes 25\nPrueba 1\n\n\n\n\nMartes 26\n\nReporte 2\n\n\n\n Octubre \n\n\n\n\n\nLunes 2\nInferencia 2\n\nMoore cap 6\n\n\nMartes 3\n\nInferencia 2\n\n\n\nLunes 9\nFeriado\n\n\n\n\nMartes 10\n\nInstrucciones trabajo final\n\n\n\nLunes 16\nInferencia 3\n\nMoore cap 7\n\n\nMartes 17\n\nInferencia 3\n\n\n\nLunes 23\nInferencia 4\n\nMoore cap 8\n\n\nMartes 24\n\nInferencia 4\n\n\n\nLunes 30\nSemana trabajo autónomo\n\n\n\n\nMartes 31\n\nSemana trabajo autónomo\n\n\n\n\n\n\n\n\n\n\n\n\n Clases\n Prácticos\n Lecturas\n\n\n\n\n Noviembre \n\n\n\n\n\nLunes 6\nEvaluación 2 (30%)\n\n\n\n\nMartes 7\n\nReporte 3\n\n\n\nLunes 13\nAsociación con categóricas 1\n\nMoore cap 9\n\n\nMartes 14\n\nAsesoría final trabajos\n\n\n\nLunes 20\nAsociación con categóricas 2\n\nRichtey cap 13\n\n\nLunes 27\nEntrega trabajo final\nPruebas recuperativas"
  }
]